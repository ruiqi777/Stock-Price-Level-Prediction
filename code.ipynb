{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we modified the program kernel7da3d56615.ipynb to predict the price of iShares ETF. In addition to modification of the program, window normalization was added to the GET_BATCH helper function in order to boost the model performance. Finally, performances of different neural networks were compared and analyzed.\n",
    "\n",
    "Note: Since model was evaluated based on validation sets, we will not present plots of prediction as well as Sharpe, CAGR and White Reality Check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Module imported\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPool1D, Dense, Activation, GlobalMaxPool1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import SimpleRNN\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9192a75f-766e-4b82-b978-8710f91a0876",
    "_uuid": "8f4fe85fe679d66bafc72b34ad57321a49b61587"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train = pd.read_csv('../input/ishareetf/iShareETF1.csv').fillna(0)\n",
    "# Convert all price to positive value\n",
    "train.update(train.select_dtypes(include=[np.number]).abs())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of data\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7b2c24c5-4639-49a9-9bf6-1be4bc7f68a2",
    "_uuid": "5611c14837704989e1b6d0cdabc3223c5eb1222d"
   },
   "outputs": [],
   "source": [
    "# Lag array function\n",
    "def lag_arr(arr, lag,fill):\n",
    "    filler = np.full((arr.shape[0],lag,1),-1)\n",
    "    comb = np.concatenate((filler,arr),axis=1)\n",
    "    result = comb[:,:arr.shape[1]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function calculates autocorrelations.\n",
    "This is the formula to calculate autocorrelation (the letter tao stands for lag, mu stands for mean and sigma stands for standard deviation):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks for division by zero, which native Python autocorrelation functions do not do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4cc60126-73cb-4eca-b5a8-7ba2cf4e8d2e",
    "_uuid": "b9fc5e09157453104fe9aa2bc0ed329b6346b85f"
   },
   "outputs": [],
   "source": [
    "# Single autocorrelation function\n",
    "def single_autocorr(series, lag):\n",
    "    \"\"\"\n",
    "    Autocorrelation for single data series\n",
    "    :param series: traffic series\n",
    "    :param lag: lag, days\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s1 = series[lag:]\n",
    "    s2 = series[:-lag]\n",
    "    ms1 = np.mean(s1)\n",
    "    ms2 = np.mean(s2)\n",
    "    ds1 = s1 - ms1\n",
    "    ds2 = s2 - ms2\n",
    "    divider = np.sqrt(np.sum(ds1 * ds1)) * np.sqrt(np.sum(ds2 * ds2))\n",
    "    return np.sum(ds1 * ds2) / divider if divider != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function calculates the autocorrelations for each series in the batch. Then we\n",
    "fuse the correlations together into one NumPy array. Since autocorrelations are a\n",
    "global feature, we need to create a new dimension for the length of the series and\n",
    "another new dimension to show that this is only one feature. We then repeat the\n",
    "autocorrelations over the entire length of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8ebf6580-c475-47e1-ac21-0bdfcbc04cee",
    "_uuid": "0ecfe613c7d95d0795f63bb5696bb98a857e2fab"
   },
   "outputs": [],
   "source": [
    "# Batch autocorrelation function\n",
    "def batc_autocorr(data,lag,series_length):\n",
    "    corrs = []\n",
    "    for i in range(data.shape[0]):\n",
    "        c = single_autocorr(data, lag) \n",
    "        corrs.append(c)\n",
    "    corr = np.array(corrs)\n",
    "    corr = corr.reshape(-1,1)\n",
    "    corr = np.expand_dims(corr,-1)\n",
    "    corr = np.repeat(corr,series_length,axis=1)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 4 cells take care of one hot encoding data: 'Region', 'Sub Asset Class' and 'Asset Class' categoricals and days categoricals pulled from the date columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f3fe8c58-5643-4147-b165-b71d4863c883",
    "_uuid": "ee8bb28ed57d8858881b56dea64eacab80c362c1"
   },
   "outputs": [],
   "source": [
    "# One hot encoding of 'region'\n",
    "region_int = LabelEncoder().fit(train['Region'])\n",
    "region_enc = region_int.transform(train['Region'])\n",
    "region_enc = region_enc.reshape(-1, 1)\n",
    "region_one_hot = OneHotEncoder(sparse=False).fit(region_enc)\n",
    "\n",
    "del region_enc\n",
    "type(region_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "97cd32b5-565b-4265-9a84-b14e4b962c7c",
    "_uuid": "721542f10a9ee51b4c0f800bced787763460fe53"
   },
   "outputs": [],
   "source": [
    "# One hot encoding of date\n",
    "datetime.datetime.strptime(train.columns.values[1], '%m/%d/%Y').strftime('%a')\n",
    "weekdays = [datetime.datetime.strptime(date,'%m/%d/%Y').strftime('%a') \n",
    "           for date in train.columns.values[:-4]]\n",
    "\n",
    "day_one_hot = LabelEncoder().fit_transform(weekdays)\n",
    "day_one_hot = day_one_hot.reshape(-1, 1)\n",
    "day_one_hot = OneHotEncoder(sparse=False).fit_transform(day_one_hot)\n",
    "day_one_hot = np.expand_dims(day_one_hot,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cac63457-7bdc-4602-bd77-04faa36684e3",
    "_uuid": "4e8ba967ad100e242901e5097ed884d7a2796309"
   },
   "outputs": [],
   "source": [
    "# One hot encoding of 'Sub Asset Class'\n",
    "sac_int = LabelEncoder().fit(train['Sub Asset Class'])\n",
    "sac_enc = sac_int.transform(train['Sub Asset Class'])\n",
    "sac_enc = sac_enc.reshape(-1, 1)\n",
    "sac_one_hot = OneHotEncoder(sparse=False).fit(sac_enc)\n",
    "\n",
    "del sac_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8d612753-2b4b-45f7-bfac-c30becd5e378",
    "_uuid": "9e31bba31ad67686dbfcc633475fa1e049e0bf5f"
   },
   "outputs": [],
   "source": [
    "# One hot encoding of 'Asset Class'\n",
    "ac_int = LabelEncoder().fit(train['Asset Class'])\n",
    "ac_enc = ac_int.transform(train['Asset Class'])\n",
    "ac_enc = ac_enc.reshape(-1, 1)\n",
    "ac_one_hot = OneHotEncoder(sparse=False).fit(ac_enc)\n",
    "\n",
    "del ac_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0e123aec-24fb-4688-b0e8-aaf8d106c458",
    "_uuid": "c4ba6c494beac030d7eac3dadcdb85d7f784b39a"
   },
   "source": [
    "This is what the function get_batch() below does:\n",
    "\n",
    "1. Ensures there is enough data to create a lookback window and a target from\n",
    "the given starting point.\n",
    "2. Separates the lookback window from the training data.\n",
    "3. Separates the target and then takes the one plus logarithm of it.\n",
    "4. Takes the one plus logarithm of the lookback window and adds a feature\n",
    "dimension.\n",
    "5. Gets the days from the precomputed one-hot encoding of days and repeats\n",
    "it for each time series in the batch.\n",
    "6. Computes the lag features for year lag, half-year lag, and quarterly lag.\n",
    "7. Encodes the global features using the preceding defined\n",
    "encoders. The next two steps, 8 and 9, will echo the same role.\n",
    "8. Repeats step 7.\n",
    "9. Repeats step 7 and 8.\n",
    "10. Calculates the year, half-year, and quarterly autocorrelation.\n",
    "11. Calculates the median for the lookback data.\n",
    "12. Fuses all these features into one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f2b98941-4744-49ad-be61-e171a8e6f09b",
    "_uuid": "c8e7a9580983f56953ee5aa4d79dbe4c44e93597"
   },
   "outputs": [],
   "source": [
    "# Get batch helper function\n",
    "def get_batch(train,start=1,lookback = 200):\n",
    "    assert((start + lookback) <= (train.shape[1] - 3)) , 'End of lookback would be out of bounds' #1\n",
    "    add=1\n",
    "    data = train.iloc[:,start:start + lookback].values #start of window is randomly chosen #2\n",
    "    target = train.iloc[:,start + lookback].values #gets one item beyond the training window\n",
    "    \n",
    "   # Window normalization\n",
    "    f = 0.1\n",
    "    small = .001\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = ((df/(np.array(df.iloc[:,0]).reshape(-1,1)+small))-1)*f\n",
    "    data = df.values\n",
    "    \n",
    "    df_target = pd.DataFrame(target)\n",
    "    df_target = ((df_target/(np.array(df.iloc[:,0]).reshape(-1,1)+small))-1)*f \n",
    "    target = df_target.values\n",
    "    \n",
    "    target = np.log1p(target)#3\n",
    "    \n",
    "    log_view = np.log1p(data)\n",
    "    log_view = np.expand_dims(log_view,axis=-1)#4\n",
    "    \n",
    "    days = day_one_hot[:,start:start + lookback]\n",
    "    days = np.repeat(days,repeats=train.shape[0],axis=0)#5\n",
    "    \n",
    "    year_lag = lag_arr(log_view,365,-1)#6\n",
    "    halfyear_lag = lag_arr(log_view,182,-1)\n",
    "    quarter_lag = lag_arr(log_view,91,-1)\n",
    "    \n",
    "    region_enc = region_int.transform(train['Region']) #7\n",
    "    region_enc = region_enc.reshape(-1, 1)\n",
    "    region_enc = region_one_hot.transform(region_enc)\n",
    "    region_enc = np.expand_dims(region_enc,1)\n",
    "    region_enc = np.repeat(region_enc,lookback,axis=1)\n",
    "    \n",
    "    sac_enc = sac_int.transform(train['Sub Asset Class'])#8\n",
    "    sac_enc = sac_enc.reshape(-1, 1)\n",
    "    sac_enc = sac_one_hot.transform(sac_enc)\n",
    "    sac_enc = np.expand_dims(sac_enc, 1)\n",
    "    sac_enc = np.repeat(sac_enc,lookback,axis=1)\n",
    "    \n",
    "    ac_enc = ac_int.transform(train['Asset Class'])#9\n",
    "    ac_enc = ac_enc.reshape(-1, 1)\n",
    "    ac_enc = ac_one_hot.transform(ac_enc)\n",
    "    ac_enc = np.expand_dims(ac_enc,1)\n",
    "    ac_enc = np.repeat(ac_enc,lookback,axis=1)\n",
    "    \n",
    "    year_autocorr = batc_autocorr(data,lag=365,series_length=lookback)#10\n",
    "    halfyr_autocorr = batc_autocorr(data,lag=182,series_length=lookback)\n",
    "    quarter_autocorr = batc_autocorr(data,lag=91,series_length=lookback)\n",
    "    \n",
    "    medians = np.median(data,axis=1) #11\n",
    "    medians = np.expand_dims(medians,-1)\n",
    "    medians = np.expand_dims(medians,-1)\n",
    "    medians = np.repeat(medians,lookback,axis=1)\n",
    "    \n",
    "    batch = np.concatenate((log_view, \n",
    "                            days, \n",
    "                            year_lag, \n",
    "                            halfyear_lag, \n",
    "                            quarter_lag,\n",
    "                            sac_enc,\n",
    "                            region_enc,\n",
    "                            ac_enc, \n",
    "                            year_autocorr, \n",
    "                            halfyr_autocorr,\n",
    "                            quarter_autocorr, \n",
    "                            medians),axis=2)#12\n",
    "    \n",
    "    return batch, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATOR FUNCTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "903d1b39-4e52-4701-bdfa-3de83eb7fa2c",
    "_uuid": "07a50ee3178189fe8013a4c6e15473f05b42e994"
   },
   "outputs": [],
   "source": [
    "# Generate batches function\n",
    "def generate_batches(train,batch_size = 1, lookback = 200):\n",
    "    num_samples = train.shape[0]\n",
    "    num_steps = train.shape[1] - 5\n",
    "    \n",
    "    while True:\n",
    "        # Loop to create batches\n",
    "        for i in range(num_samples // batch_size):\n",
    "            batch_start = i * batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "            \n",
    "            #gets a random date (column number) from where to start the window of length lookback\n",
    "            seq_start = np.random.randint(num_steps - lookback) \n",
    "            \n",
    "            # Generate batch and target using get_batch\n",
    "            X,y = get_batch(train.iloc[batch_start:batch_end],start=seq_start)\n",
    "            \n",
    "            # Yield iterators\n",
    "            yield X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZE VARIABLES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "62a3d56a-3cf8-4765-89a1-6925d6e40be1",
    "_uuid": "92324440e7b3a39440d3a65376282293093ad97d"
   },
   "outputs": [],
   "source": [
    "#=timesteps=lookback\n",
    "max_len = 200 \n",
    "n_features = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA INPUT START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d83c087-d814-4693-86f2-6ea870b9300e",
    "_uuid": "c4bb2b93638f5c05d9721d9015be840b33631a74"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# Data Split\n",
    "train_df, val_df = train_test_split(train, test_size=0.1)\n",
    "\n",
    "# Training and validation iterators\n",
    "train_gen = generate_batches(train_df,batch_size=batch_size) #train_gen is a  batch cube\n",
    "val_gen = generate_batches(val_df, batch_size=batch_size) #val_gen is a batch cube\n",
    "n_train_samples = train_df.shape[0]\n",
    "n_val_samples = val_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes of training and validation data\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPILE convolutional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setting\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(16,5, input_shape=(max_len,n_features)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool1D(5))\n",
    "\n",
    "model.add(Conv1D(16,5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool1D(5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c82a220a-054b-41df-91dc-c149b41ddbf1",
    "_uuid": "ff828efc68dfa21cc072daf3bacd25ddf9af85cc"
   },
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "hist = model.fit_generator(train_gen, \n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=n_train_samples // batch_size, \n",
    "                    validation_data= val_gen, \n",
    "                    validation_steps=n_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(hist.history['val_loss']))\n",
    "Convolutional_loss=hist.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPILE SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setting\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(16,input_shape=(max_len,n_features)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "hist = model.fit_generator(train_gen, \n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=n_train_samples // batch_size, \n",
    "                    validation_data= val_gen, \n",
    "                    validation_steps=n_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.history['val_loss'])\n",
    "SimpleRNN_loss=hist.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPILE ComplexRNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setting\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32,return_sequences=True,input_shape=(max_len,n_features)))\n",
    "model.add(SimpleRNN(16, return_sequences = True))\n",
    "model.add(SimpleRNN(16))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "hist = model.fit_generator(train_gen, \n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=n_train_samples // batch_size, \n",
    "                    validation_data= val_gen, \n",
    "                    validation_steps=n_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS ComplexRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.history['val_loss'])\n",
    "SimpleRNNagain_loss=hist.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPILE CuDNNLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5f44c453-5f9c-4e13-ba2c-8ab36a268c62",
    "_uuid": "c48edcfe20c92443f4e6c64ed42a8846bb3485c8"
   },
   "outputs": [],
   "source": [
    "# Model setting\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(16,input_shape=(max_len,n_features)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8eee4ae3-19f7-4e96-9e88-7d66852fe637",
    "_uuid": "f8b6859f15cbec0acb6a6c71d0695c304a666530"
   },
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "hist = model.fit_generator(train_gen, \n",
    "                    epochs=1,\n",
    "                      steps_per_epoch=n_train_samples // batch_size, \n",
    "                    validation_data= val_gen, \n",
    "                    validation_steps=n_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS CuDNNLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.history['val_loss'])\n",
    "CuDNNLSTM_loss=hist.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we calculate the Mad/Mean Ratios of our NN results. We have avoided using MAPE, instead we have calculated MAD. Next we calculate MAD/Mean by dividing MAD by the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of validation dataset\n",
    "df = np.log1p(val_df.iloc[:,1:1257]) \n",
    "print(df.describe().transpose())\n",
    "dfd=df.describe().transpose()\n",
    "\n",
    "# Calculate Mad/MeanRatio\n",
    "Convolutional_Mad_MeanRatio = Convolutional_loss/dfd['mean'].mean()\n",
    "SimpleRNN_Mad_MeanRatio = SimpleRNN_loss/dfd['mean'].mean()\n",
    "SimpleRNNagain_Mad_MeanRatio = SimpleRNNagain_loss/dfd['mean'].mean()\n",
    "CuDNNLSTM_Mad_MeanRatio = CuDNNLSTM_loss/dfd['mean'].mean()\n",
    "\n",
    "# Mad/MeanRatio results\n",
    "print(\"Convolutional_Mad_MeanRatio\", Convolutional_Mad_MeanRatio)\n",
    "print(\"SimpleRNN_Mad/MeanRatio\",SimpleRNN_Mad_MeanRatio)\n",
    "print(\"SimpleRNNagain_Mad/MeanRatio\", SimpleRNNagain_Mad_MeanRatio)\n",
    "print(\"CuDNNLSTM_Mad/MeanRatio\", CuDNNLSTM_Mad_MeanRatio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
